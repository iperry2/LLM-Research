import os
import re
from num2words import num2words
import pandas as pd
import numpy as np
from PyPDF2 import PdfReader
import arxiv
from transformers import pipeline, AutoTokenizer

#function to extract text from pdf
def extract_text_from_pdf(pdf_path):
    try:
        # Check if the file exists and print its size
        if not os.path.exists(pdf_path):
            print(f"Error: The file at {pdf_path} does not exist.")
            return []
        
        file_size = os.path.getsize(pdf_path)
        if file_size == 0:
            print(f"Error: The file at {pdf_path} is empty.")
            return []
        
        reader = PdfReader(pdf_path)
        text = []
        for page in reader.pages:
            page_text = page.extract_text()
            if page_text:
                text.append(page_text)
            else:
                print(f"Warning: No text extracted from a page in the file at {pdf_path}.")
        return text
    except Exception as e:
        print(f"An unexpected error occurred: {e}")
        return []

def clean_text(text):
    # Define patterns for the main sections
    intro_pattern = re.compile(r'\bINTRODUCTION\b', re.IGNORECASE)
    ack_pattern = re.compile(r'\bACKNOWLEDGMENTS?\b', re.IGNORECASE)
    refs_pattern = re.compile(r'\bREFERENCES\b', re.IGNORECASE)
    
    # Find the positions of these sections
    intro_match = intro_pattern.search(text)
    ack_match = ack_pattern.search(text)
    refs_match = refs_pattern.search(text)
    
    # Determine start and end positions
    start_pos = intro_match.start() if intro_match else 0
    end_pos = len(text)
    
    if ack_match:
        end_pos = ack_match.start()
    elif refs_match:
        end_pos = refs_match.start()
    
    # Extract and return the body text
    body_text = text[start_pos:end_pos].strip()
    return body_text

# Function to fetch and print abstract for a specific arXiv ID
def get_arxiv_abstract(arxiv_id):
    search = arxiv.Search(id_list=[arxiv_id])
    
    for result in search.results():
        print('\n')
        print("Title: ", result.title)
        print('\n')
        print("Abstract: ", result.summary)
        print('\n')
        print("Authors: ", result.authors)
        print('\n')
        print("PDF URL: ", result.pdf_url)
        print("\n")

# Function to normalize text (s is input text)
def normalize_text(s, sep_token = " \n "):
    s = re.sub(r'\s+',  ' ', s).strip()
    s = re.sub(r". ,","",s)
    # remove all instances of multiple spaces
    s = s.replace("..",".")
    s = s.replace(". .",".")
    s = s.replace("\n", "")
    s = s.strip()
    return s

# use FalconsAI to get summary
def summarize_text(text):
    summarizer = pipeline("summarization", model="Falconsai/text_summarization")

    # Ensure that the input is either a string or a list of strings
    if not isinstance(text, str) and not all(isinstance(t, str) for t in text):
        raise ValueError("Input must be either a string or a list of strings.")

    # Check if the input is a list of strings
    if isinstance(text, list):
        text = "\n".join(text)

    max_length = min(2 * len(text), 7)  # Adjust max_length based on input length, up to a maximum of 200
    min_length = int(min(0.5 * len(text), 5))  # Set min_length to 80% of the input length or 25, whichever is larger
    summary = summarizer(text, max_length=max_length, min_length=min_length, do_sample=False)
    return summary



def main():
    # ArXiv abstract from the paper with ID 2405.14945
    get_arxiv_abstract("2405.14945")

    # Extract text from PDF
    pdf_file = "/Users/ian/VS-Projects/TextSummary-FalconAI(HF)/Sample-DataFrame/BlackHole.pdf"
    
    # Check if the file exists
    if not os.path.exists(pdf_file):
        print(f"Error: The file at {pdf_file} does not exist.")
        return
    
    # Define the summarizer and tokenizer using transformers library
    summarizer = pipeline("summarization", model="Falconsai/text_summarization")

    text = extract_text_from_pdf(pdf_file)

    # If the file couldn't be read, exit the script
    if not text:
        print(f"Error: No text extracted from the file at {pdf_file}.")
        return

    # Normalize text
    normalized_text = [normalize_text(page_text) for page_text in text]

    # Join normalized text into a single string
    combined_text = " ".join(normalized_text)

    # Clean the text and remove unwanted summaries using trigger words
    cleaned_text = clean_text(combined_text)

    # Initialize variables for summarizing chunks of text
    max_chunk_size = 500  # Choose an appropriate chunk size (based on model limits)
    start_idx = 0
    summaries = []

    # Process each chunk of text
    while start_idx < len(cleaned_text):
        # Select a chunk of the text
        text_chunk = cleaned_text[start_idx:start_idx + max_chunk_size]
        #output_file.write(f"Processing chunk from index {start_idx} to {start_idx + max_chunk_size}"  + "\n")  # Debug print

        # Summarize the chunk of text
        summary_result = summarizer(text_chunk, max_length=44 , min_length=10 , do_sample=False)
        full_summary = ""
            
        # Collect the summary
        if summary_result:
            summaries.append(summary_result[0]["summary_text"])
            start_idx += max_chunk_size
        
            # Combine the summaries for the current PDF
            full_summary += "\n".join(summaries) + "\n\n"

    # Print the Summary
    #summary = summarize_text(normalized_text)
    print("Summary: ", full_summary)

if __name__ == "__main__":
    main()
